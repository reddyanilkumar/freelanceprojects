>>> dataPath = sc.textFile("/home/anil/Anil/Ben/ProductPartMatrix.csv")
>>> header
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'header' is not defined
>>> header = dataPath.first()
>>> dataFiltered = dataPath.filter(lambda line : line != header)
>>> dataFiltered.count()
689544                                                                          
>>> dataUnique = dataFiltered.map(lambda line : (line.split(",")[0],line.split(",")[1])).distinct()
>>> dataUnique.take(2)
[(u'917-00406-001-015', u'244-00001-002-047'), (u'902-00215-130-049', u'246-05001-151-047')]
>>> dataUnique.count()
496672                                                                          
>>> 
from pyspark.mllib.linalg import Vectors
rows = dataUnique.groupByKey().map(lambda p : Vectors.sparse(productsAmount, p._2.map(r=>(r._1-1, r._2)).toSeq))
data = sqlContext.read.option("header","true").csv("/home/anil/Anil/Ben/ProductPartMatrix.csv").withColumnRenamed("BOM No.","BOM").withColumnRenamed("Part No.","Part").distinct()
>>> data.count()
689544                                                                          
from pyspark.ml.feature import StringIndexer
indexer = StringIndexer(inputCol="BOM", outputCol="BOM Cat") 
indexer1 = StringIndexer(inputCol="Part", outputCol="Part Cat")
data = data.na.drop(subset="Part")
data = indexer.fit(data).transform(data)
data = indexer1.fit(data).transform(data)
from pyspark.mllib.linalg.distributed import RowMatrix
from pyspark.ml.feature import IndexToString, StringIndexer
mat = RowMatrix(rows)
e 'lit' is not defined
>>> from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix
converter = IndexToString(inputCol="user", outputCol="BOM")
converter1 = IndexToString(inputCol="product", outputCol="Part")

from pyspark.sql.functions import lit
data = data.withColumn("action", lit(1))
indexedFinal.limit(100).rdd.map(lambda x : Vectors.dense(x)).collect()
indexedRDD = indexedFinal.limit(100).rdd.map(lambda x : Vectors.dense(x)).zipWithIndex().map(lambda (value, index) : IndexedRow(index, value))
matrix = IndexedRowMatrix(indexedRDD)
matrix1 = matrix.toCoordinateMatrix().transpose().toIndexedRowMatrix()

model = ALS.trainImplicit(data['BOM Cat','Part Cat','action'].limit(100), 10, 10, 0.01, -1, 0.1)
predictions = model.predictAll(finaldata['BOM Cat','Part Cat'].limit(100).rdd)
predictions = predictions.toDF().withColumnRenamed("user","BOM Cat").withColumnRenamed("product","Part Cat")
predictions = data.join(predictions, [predictions["BOM Cat"] == data["BOM Cat"],predictions["Part Cat"] == data["Part Cat"]], "inner")
finalout = predictions['BOM','Part','rating']
predictions.filter(predictions['rating']>1).collect()

from pyspark.sql.types import DecimalType
from pyspark.mllib.linalg.distributed import MatrixEntry


